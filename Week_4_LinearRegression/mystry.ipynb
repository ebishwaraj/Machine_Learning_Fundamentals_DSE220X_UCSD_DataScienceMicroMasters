{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Finding features with most relevance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file  **mystery.dat**  contains pairs  (x,y) , where  x∈R100  and  y∈R . There is one data point per line, with comma-separated values; the very last number in each line is the  y -value.\n",
    "\n",
    "In this data set,  y  is a linear function of just ten of the features in  x , plus some noise. Your job is to identify those ten features.\n",
    "\n",
    "Which of the following contain only relevant features?\n",
    "\n",
    "(Think of the feature numbers as being in the range 1 to 100, but be aware that Python indexes arrays starting at zero.)\n",
    "> 1,5,7,19,44 \\\n",
    "> 2,3,13,17,29 \\\n",
    "> 3,7,13,19,44 \\\n",
    "> 5,23,24,51,61 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14558, -1.29249,  0.84911, ...,  1.5532 , -1.42135,  1.19238],\n",
       "       [ 1.38724, -1.00201, -0.3337 , ...,  0.81903,  0.39286, -3.44094],\n",
       "       [ 1.47233,  0.8488 , -0.33866, ...,  0.08911, -1.72476,  3.75006],\n",
       "       ...,\n",
       "       [-0.83673,  0.80514,  0.00807, ..., -1.64165,  2.04662,  1.84121],\n",
       "       [ 1.12062,  0.68561, -1.08   , ...,  1.1926 ,  0.33696,  3.53143],\n",
       "       [-0.28943, -0.22213, -0.52226, ...,  0.22531,  1.72576, -0.55118]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "data = np.genfromtxt('mystery.dat', delimiter=',')\n",
    "#Let's see data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0:100]\n",
    "y = data[:,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print(\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 99):\n",
    "            print(\"Feature index is out of bounds\")\n",
    "            return\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return  mean_squared_error(y, regr.predict(x[:,flist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature list =  [0, 4, 6, 18, 43] MSE =  6.666805964795619\n",
      "feature list =  [1, 2, 12, 16, 28] MSE =  5.931005043656825\n",
      "feature list =  [2, 6, 12, 18, 43] MSE =  6.711275934495151\n",
      "feature list =  [4, 22, 23, 50, 60] MSE =  7.497074872393358\n"
     ]
    }
   ],
   "source": [
    "#subtract 1 from each feature number as feature no start from 0 on python\n",
    "features = [[0,4,6,18,43],[1,2,12,16,28],[2,6,12,18,43],[4,22,23,50,60]]\n",
    "\n",
    "for flist in features:\n",
    "    print('feature list = ', flist, 'MSE = ', feature_subset_regression(x,y,flist))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that features [2,3,13,17,29] have lower MSE compared to other features, but that does not necessarily mean **ONLY** relevant features are included in this list.\n",
    "\n",
    "We can confirm that by finding the list of top 10 relevant/least MSE features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_feature_regression(x,y,f):\n",
    "    if (f < 0) or (f > 99):\n",
    "        print(\"Feature index is out of bounds\")\n",
    "        return\n",
    "    regr = linear_model.LinearRegression()\n",
    "    x1 = x[:,[f]]\n",
    "    regr.fit(x1, y)\n",
    "    # Make predictions using the model\n",
    "    y_pred = regr.predict(x1)\n",
    "    return mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_mse_features(x,y,n):\n",
    "    ls=[]\n",
    "    indices=[]\n",
    "    for i in range(0,100):\n",
    "        ls.append(one_feature_regression(x,y,i))\n",
    "    sorted_ls=sorted(ls)\n",
    "    #[i+1 for i,x in enumerate(ls) if x=min(ls)]\n",
    "    for x in sorted_ls:\n",
    "        indices.append(ls.index(x))\n",
    "    return indices[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For features =  [16, 10, 18, 22, 12, 25, 4, 80, 6, 1] +1 MSE =  2.135608858468031\n"
     ]
    }
   ],
   "source": [
    "top_10 = lowest_mse_features(x,y,10)\n",
    "print('For features = ', top_10, '+1', 'MSE = ', feature_subset_regression(x,y,top_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
